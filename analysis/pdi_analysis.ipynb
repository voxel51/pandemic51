{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDI Analysis\n",
    "\n",
    "This script investigates the output of our live stream detections in order to gain useful insights.\n",
    "The goal is to create a \"Physical Distancing Index\" (`PDI`) that shows the trends and changes at\n",
    "a camera feed while removing noise and perturbations from unwanted effects.\n",
    "\n",
    "In this directory is a CSV file containing historical data for 7 cams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's read in the data.\n",
    "\n",
    "There are three columns in the data:\n",
    "- `stream_name`: the source camera location\n",
    "- `timestamp`: the epoch time (in seconds) when the camera was sampled\n",
    "- `count`: the number of objects detected in the image (people, cars, bicycles, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"counts.csv\")\n",
    "\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can reorganize the data by stream and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    stream_name: {\n",
    "        \"timestamp\": data[data[\"stream_name\"]==stream_name][\"timestamp\"],\n",
    "        \"count\": data[data[\"stream_name\"]==stream_name][\"count\"]\n",
    "    } for stream_name in sorted(list(set(data[\"stream_name\"])))\n",
    "}\n",
    "\n",
    "print(\"Stream sources:\")\n",
    "print(list(data_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was not sampled uniformly. Sampling is denser starting around mid-march and there appears to be a gap for a day or two in late march.\n",
    "\n",
    "Still, we immediately can start to see some interesting things.\n",
    "\n",
    "First off, there are far more detections on the `time_square` feed than any other, which is reassuring.\n",
    "\n",
    "We also notice a rather dramatic trend of decreasing count around mid-march.\n",
    "\n",
    "Despite the trend, there is still a lot of noise in the data. Even before the pandemic, there are times of day that a camera may not be busy. We can certainly clean up this signal for a clearer and more meaningful interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 7))\n",
    "for stream_name, d in data_dict.items():\n",
    "    timestamps, counts = d[\"timestamp\"], d[\"count\"]\n",
    "    datetimes = pd.to_datetime(timestamps, unit='s')\n",
    "    plt.scatter(datetimes, counts, label=stream_name, alpha=0.7)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"object count\")\n",
    "plt.xlim([pd.Timestamp('2020-02-01'), pd.Timestamp('2020-04-03')])\n",
    "plt.ylim([0, 100])\n",
    "plt.title(\"Detected Object Count VS Time for Live Streams Cameras\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function will allow us to quickly visualize our candidate `PDI` statistics.\n",
    "\n",
    "One implementation detail to mention here is that this plot function normalizes the magnitude of the PDI for each stream, such that they all have a maximum of 100. This is because the the overall magnitude is heavily dependent on both the activity in the city and more specifically the activity at the exact location of the camera, but what we really care about is relative change. This will help us with visualizing our different statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pdi(data_dict, pdi_func):\n",
    "    plt.figure(figsize=(18, 7))\n",
    "\n",
    "    for stream_name, d in data_dict.items():\n",
    "        timestamps, counts = d[\"timestamp\"], d[\"count\"]\n",
    "        datetimes = pd.to_datetime(timestamps, unit='s')\n",
    "        datetimes, pdis = pdi_func(datetimes, counts)\n",
    "        # normalize PDIs\n",
    "        normalized_pdis = np.array(pdis) * 100 / np.max(pdis)\n",
    "        plt.plot(datetimes, normalized_pdis, label=stream_name)\n",
    "\n",
    "    plt.xlabel(\"data\")\n",
    "    plt.ylabel(\"number of detected objects\")\n",
    "    plt.xlim([pd.Timestamp('2020-02-01'), pd.Timestamp('2020-04-03')])\n",
    "    plt.ylim([0, 100])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clean up the noise in this signal by averaging over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pdi_func(window_days, avg_fcn):\n",
    "    def pdi_func(datetimes, counts):\n",
    "        pdis = np.zeros(len(counts))\n",
    "\n",
    "        for i, time in enumerate(datetimes):\n",
    "            # get all counts within the temporal window\n",
    "            start_time = time - pd.Timedelta(days=window_days)\n",
    "            window_counts = counts[(start_time <= datetimes) & (datetimes <= time)]\n",
    "\n",
    "            # average over this window\n",
    "            pdis[i] = avg_fcn(window_counts)\n",
    "\n",
    "        return datetimes, pdis\n",
    "\n",
    "    return pdi_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's look at all counts in the past **1 day** and **average** them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of days to average counts over\n",
    "WINDOW_DAYS = 1\n",
    "\n",
    "# a function that averages the counts within a window\n",
    "AVG_FCN = np.mean\n",
    "\n",
    "pdi_func_v1 = create_pdi_func(WINDOW_DAYS, AVG_FCN)\n",
    "\n",
    "plot_pdi(data_dict, pdi_func_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm...not great, but perhaps we're on to something. We can increase the averaging window to **3 days** and instead take the $L^2$ **(euclidean) norm**.\n",
    "\n",
    "The [$L^p$ norm](https://en.wikipedia.org/wiki/Lp_space) weighs each sample by its magnitude. The higher $p$ is, the more weight is given to larger values. $L^1$ is equivalent to the mean (each sample has the same weight). $L^\\infty$ is equivalent to choosing the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of days to average counts over\n",
    "WINDOW_DAYS = 3\n",
    "\n",
    "# p-norm value for normalizing (LP=2 -> euclidean norm)\n",
    "LP = 2\n",
    "\n",
    "# a function that averages the counts within a window\n",
    "AVG_FCN = lambda x: np.linalg.norm(x, ord=LP) / (len(x) ** (1 / LP)) \n",
    "\n",
    "pdi_func_v2 = create_pdi_func(WINDOW_DAYS, AVG_FCN)\n",
    "\n",
    "plot_pdi(data_dict, pdi_func_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's looking a better! We can take the output from `pdi_func_v2` and do a bit of further processing.\n",
    "\n",
    "Using a simple rectangular filter we can smooth out the plot. In the code below, a larger `SMOOTHING_WIDTH` will smooth out the jaggedness but at a cost of reducing the resolution of transients, i.e. smearing any abrupt changes over a longer time.\n",
    "\n",
    "`40` seems like a pretty good value, but feel free to play with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOOTHING_WIDTH = 40\n",
    "\n",
    "def pdi_func_v3(datetimes, counts):\n",
    "    datetimes, pdis = pdi_func_v2(datetimes, counts)\n",
    "    \n",
    "    if SMOOTHING_WIDTH:\n",
    "        kernel_size = min(SMOOTHING_WIDTH, len(pdis))\n",
    "        kernel = np.ones(kernel_size) / kernel_size\n",
    "        pdis = list(np.convolve(pdis, kernel, mode=\"same\"))\n",
    "\n",
    "    return datetimes, pdis\n",
    "\n",
    "plot_pdi(data_dict, pdi_func_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theta3",
   "language": "python",
   "name": "theta3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
